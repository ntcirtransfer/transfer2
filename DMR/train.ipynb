{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import os, importlib, pickle, time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import dpr_models\n",
    "importlib.reload(dpr_models)\n",
    "\n",
    "# Define run name\n",
    "run_name = 'baseline'\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Use {device}')\n",
    "# Check if MPS is available\n",
    "#device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "# Define model configration\n",
    "configs = {'lifelog_dir':'./Lifelog-6/',\n",
    "           'embedding_dim':512,\n",
    "           'hidden_dim':512,\n",
    "           'batch_size':512,\n",
    "           'epochs':10,\n",
    "           'N_valid':200,\n",
    "           'sensor_feats':['heart_rate(bpm)','heart_rate_conf','calories','distance',\n",
    "                           'minutesAsleep','minutesAwake','minutesAfterWakeup','timeInBed','sleep_efficiency',\n",
    "                           'new_lat','new_lng','semantic_name','categories','movement','city'],\n",
    "           'normalization':True,\n",
    "           'model_path':f'./models/{run_name}.pth',\n",
    "           'configs_path':f'./models/{run_name}.pkl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sensor data\n",
    "lifelog_dir = configs['lifelog_dir']\n",
    "\n",
    "df_sen = pd.read_csv(f'{lifelog_dir}/lsc22_metadata.csv', low_memory=False)\n",
    "df_loc = pd.read_csv(f'{lifelog_dir}/vaisl_gps.csv', low_memory=False)\n",
    "df_loc = df_loc.drop_duplicates()\n",
    "df = pd.merge(df_loc, df_sen, how='left', on='minute_id', suffixes=('', '_y'))\n",
    "\n",
    "# Fix image path\n",
    "df['month'] = df['ImageID'].map(lambda x:x[0:6])\n",
    "df['day'] = df['ImageID'].map(lambda x:x[6:8])\n",
    "df['ImageID_full'] = lifelog_dir + '/images/' + df['month'] + '/' + df['day'] + '/' + df['ImageID']\n",
    "\n",
    "df = df[(df['month']=='202003')]\n",
    "\n",
    "# Check image file existence\n",
    "df['image_file_exist'] = df['ImageID_full'].map(lambda x: os.path.exists(x))\n",
    "df = df[df['image_file_exist']]\n",
    "\n",
    "# Sampling data\n",
    "df['local_time'] = pd.to_datetime(df['ImageID'], format='%Y%m%d_%H%M%S_000.jpg')\n",
    "df_sampled = (\n",
    "    df.set_index('local_time') # Set index \n",
    "      .groupby(pd.Grouper(freq='H')) # Set freq to hourly \n",
    "      .apply(lambda x: x.sample(n=min(20, len(x)), random_state=0))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Frequency encoding for categorical data\n",
    "for col in df[configs['sensor_feats']].select_dtypes(['object']):\n",
    "    df_sampled[col] = df_sampled[col].fillna('Unknown')\n",
    "    df_gp = df_sampled.groupby(col)[col].count().reset_index(name=f'{col}_freq')\n",
    "    df_gp[f'{col}_fe'] = df_gp[f'{col}_freq'] / df_gp[f'{col}_freq'].sum()\n",
    "    df_sampled = pd.merge(df_sampled, df_gp, how='left', on=col)\n",
    "    df_sampled[f'{col}_orig'] = df_sampled[col].copy()\n",
    "    df_sampled[col] = df_sampled[f'{col}_fe'].copy()\n",
    "    configs[col] = dict(zip(df_gp[col], df_gp[f'{col}_fe']))\n",
    "\n",
    "sensor_data = df_sampled[configs['sensor_feats']].fillna(0.).values\n",
    "sensor_data = torch.from_numpy(sensor_data.astype(np.float32)).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image data\n",
    "images_list = []\n",
    "for image_path in df_sampled['ImageID_full'].tolist():\n",
    "    image = io.read_image(path=image_path)\n",
    "    image = transforms.functional.resize(img=image, size=(224,224)).to(torch.float32)\n",
    "    image = transforms.functional.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    images_list.append(image)\n",
    "image_data = torch.stack(images_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of data\n",
    "print('Image data: ', image_data.shape)\n",
    "print('Snesor data:', sensor_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class for image and sensor\n",
    "class ImageSensorDataset(Dataset):\n",
    "    def __init__(self, image_data, sensor_data):\n",
    "        self.image_data = image_data\n",
    "        self.sensor_data = sensor_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sensor_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = self.image_data[idx]\n",
    "        sensors = self.sensor_data[idx]\n",
    "\n",
    "        return images, sensors\n",
    "\n",
    "N_valid = configs['N_valid']\n",
    "N_train = int(len(image_data)) - N_valid\n",
    "perms = np.random.permutation(len(image_data))\n",
    "train_idx = perms[:N_train]\n",
    "valid_idx = perms[N_train:]\n",
    "\n",
    "train_image_data = image_data[train_idx]\n",
    "valid_image_data = image_data[valid_idx]\n",
    "train_sensor_data = sensor_data[train_idx]\n",
    "valid_sensor_data = sensor_data[valid_idx]\n",
    "\n",
    "train_dataset = ImageSensorDataset(train_image_data, train_sensor_data)\n",
    "valid_dataset = ImageSensorDataset(valid_image_data, valid_sensor_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=configs['batch_size'], shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=configs['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DPR model\n",
    "configs['sensor_avg'] = torch.mean(sensor_data, 0)\n",
    "configs['sensor_std'] = torch.std(sensor_data, 0)\n",
    "sensor_avg = torch.mean(sensor_data, 0)\n",
    "sensor_std = torch.std(sensor_data, 0)\n",
    "image_encoder = dpr_models.ImageEncoder(output_dim=configs['embedding_dim']).to(device)\n",
    "sensor_encoder = dpr_models.SensorEncoder(avg=configs['sensor_avg'].to(device),\n",
    "                                          std=configs['sensor_std'].to(device),\n",
    "                                          input_dim=sensor_data.shape[1],\n",
    "                                          hidden_dim=configs['hidden_dim'],\n",
    "                                          output_dim=configs['embedding_dim'],\n",
    "                                          normalization=configs['normalization']).to(device)\n",
    "\n",
    "dpr_model = dpr_models.DPRModel(image_encoder, sensor_encoder).to(device)\n",
    "optimizer = optim.Adam(dpr_model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = dpr_models.SimilarityBasedCrossEntropy(temperature=0.5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric (Mean Reciprocal Rank)\n",
    "def calculate_mrr(image_embeddings, sensor_embeddings):\n",
    "    image_embeddings_np = torch.cat(image_embeddings).detach().cpu().numpy()\n",
    "    sensor_embeddings_np = torch.cat(sensor_embeddings).detach().cpu().numpy()\n",
    "    similarities = 1 - pairwise_distances(image_embeddings_np, sensor_embeddings_np, metric='cosine')\n",
    "\n",
    "    num_samples = similarities.shape[0]\n",
    "    ranks = np.zeros(num_samples)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Rank the similarities\n",
    "        sorted_indices = np.argsort(-similarities[i])\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1  # +1 for 1-based rank\n",
    "        ranks[i] = rank\n",
    "    \n",
    "    mean_rank = np.mean(ranks)\n",
    "    mrr = np.mean(1 / ranks)\n",
    "    \n",
    "    return mean_rank, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "df_train = pd.DataFrame()\n",
    "for epoch in range(configs['epochs']):\n",
    "    start = time.time()\n",
    "    dpr_model.train()\n",
    "    train_loss, valid_loss = 0., 0.\n",
    "    train_image_embeddings, train_sensor_embeddings = [], []\n",
    "    valid_image_embeddings, valid_sensor_embeddings = [], []\n",
    "    for images, sensors in train_data_loader:\n",
    "        # Forward pass\n",
    "        image_emb, sensor_emb = dpr_model(images.to(device), sensors.to(device))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(image_emb, sensor_emb)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        train_image_embeddings.append(image_emb)\n",
    "        train_sensor_embeddings.append(sensor_emb)\n",
    "\n",
    "    dpr_model.eval()\n",
    "    for images, sensors in valid_data_loader:\n",
    "        # Forward pass\n",
    "        image_emb, sensor_emb = dpr_model(images.to(device), sensors.to(device))\n",
    "        loss = criterion(image_emb, sensor_emb)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        valid_image_embeddings.append(image_emb)\n",
    "        valid_sensor_embeddings.append(sensor_emb)\n",
    "\n",
    "    end = time.time()\n",
    "    time_diff = end - start\n",
    "\n",
    "    # Calculate evaluation metrics (Mean Reciprocal Rank)\n",
    "    train_mean_rank, train_mrr = calculate_mrr(train_image_embeddings, train_sensor_embeddings)\n",
    "    valid_mean_rank, valid_mrr = calculate_mrr(valid_image_embeddings, valid_sensor_embeddings)\n",
    "\n",
    "    result = f'Epoch [{epoch+1}/{configs[\"epochs\"]}]'\n",
    "    result += f', Train Loss: {train_loss:.4f}, Train Mean Rank: {train_mean_rank:.1f}/{len(train_image_data)}, Train MRR: {train_mrr:.4f}'\n",
    "    result += f', Valid Loss: {valid_loss:.4f}, Valid Mean Rank: {valid_mean_rank:.1f}/{len(valid_image_data)}, Valid MRR: {valid_mrr:.4f}'\n",
    "    result += f', Time: {time_diff:.2f}'\n",
    "\n",
    "    res = {'epoch':[epoch+1], 'N_train':N_train, 'N_valid':N_valid,\n",
    "           'train_loss':train_loss, 'train_mean_rank':train_mean_rank, 'train_mrr':train_mrr,\n",
    "           'valid_loss':valid_loss, 'valid_mean_rank':valid_mean_rank, 'valid_mrr':valid_mrr}\n",
    "    \n",
    "    df_train = pd.concat([df_train, pd.DataFrame(res)])\n",
    "    print(result)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "df_train.to_csv(f'./logs/train_log_{run_name}.csv', index=False)\n",
    "\n",
    "# Save model\n",
    "torch.save(dpr_model.state_dict(), configs['model_path'])\n",
    "\n",
    "# Save configs\n",
    "with open(configs['configs_path'], mode='wb') as f:\n",
    "    pickle.dump(configs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MRR in each epoch\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.lineplot(data=df_train, x='epoch', y='train_mrr', label='Training')\n",
    "sns.lineplot(data=df_train, x='epoch', y='valid_mrr', label='Validation')\n",
    "ax.set_ylabel('Mean reciprocal rank')\n",
    "plt.grid(axis='y')\n",
    "plt.savefig(f'./logs/mrr_{run_name}.pdf', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saved model and configs\n",
    "with open(configs['configs_path'], mode='rb') as f:\n",
    "    configs_test = pickle.load(f)\n",
    "\n",
    "image_encoder_test = dpr_models.ImageEncoder(output_dim=configs_test['embedding_dim']).to(device)\n",
    "sensor_encoder_test = dpr_models.SensorEncoder(avg=configs_test['sensor_avg'].to(device),\n",
    "                                               std=configs_test['sensor_std'].to(device),\n",
    "                                               input_dim=len(configs_test['sensor_feats']),\n",
    "                                               hidden_dim=configs_test['hidden_dim'],\n",
    "                                               output_dim=configs_test['embedding_dim']).to(device)\n",
    "dpr_model_test = dpr_models.DPRModel(image_encoder_test, sensor_encoder_test).to(device)\n",
    "\n",
    "dpr_model_test.load_state_dict(torch.load(configs_test['model_path'], weights_only=True))\n",
    "\n",
    "dpr_model_test.eval()\n",
    "image_emb, sensor_emb = dpr_model_test(images.to(device), sensors.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the model training in the background, convert the jupyter notebook to a Python file using the following command.\n",
    "```\n",
    "jupyter nbconvert --to python train.ipynb \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
